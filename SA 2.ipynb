{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvData = pd.read_csv('data\\corpus\\corpus.csv',sep=\",\")\n",
    "csvData.groupby('sentiment').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumerKey = \"\"\n",
    "consumer_secret= \"\"\n",
    "accessToken=\"\"\n",
    "accessTokenSecret = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "api = twitter.Api(consumer_key=consumerKey,consumer_secret = consumer_secret, access_token_key = accessToken, access_token_secret= accessTokenSecret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CreateTwitterData(searchString): \n",
    "    try:\n",
    "        tweetData=[]\n",
    "        tweets = api.GetSearch(searchString, count=100, result_type=\"recent\")\n",
    "        print (\"Found \"+ str(len(tweets)) + \" Tweets mentioning \" + searchString + \"!\" )      \n",
    "        #print(type(tweets))        \n",
    "        [tweetData.append(status.text)  for status in tweets]\n",
    "        return tweetData\n",
    "    except:            \n",
    "        print (\"An error occured!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter search term\"Fun\"\n",
      "Found 100 Tweets mentioning \"Fun\"!\n"
     ]
    }
   ],
   "source": [
    "search_string = input(\"Enter search term\")\n",
    "testData = CreateTwitterData(search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = csvData['content']\n",
    "Y_train = csvData['sentiment']\n",
    "X_test = pd.Series(testData[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##proTweets = []\n",
    "\n",
    "##for index,tweet in X_test.iteritems():\n",
    "##    tweet = tweet.lower()\n",
    "##    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet)\n",
    "##    tweet= re.sub('@[^\\s]+','AT_USER', tweet)\n",
    "##    tweet = re.sub(r'#([^\\s]+)',r'\\l', tweet)\n",
    "##    print(tweet)    \n",
    "##    proTweets.append(tweet)\n",
    "    \n",
    "##processedTweets = pd.Series(proTweets)  \n",
    "##print(proTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocess \n",
    "\n",
    "def preprocessTweets(tweetData):\n",
    "    processedTweets = pd.Series()\n",
    "    tweetList = []\n",
    "    for index,tweet in tweetData.iteritems():        \n",
    "        print(tweet)       \n",
    "        tweet = tweet.lower()\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet)\n",
    "        tweet= re.sub('@[^\\s]+','AT_USER', tweet)\n",
    "        tweet = re.sub(r'#([^\\s]+)',r'\\l', tweet)\n",
    "        #tweet=word_tokenize(tweet)\n",
    "        \n",
    "        #swords = set(stopwords.words('english')+list(punctuation)+['AT_USER','URL'])\n",
    "        #processedTweets.append(([word for word in tweet if word not in swords], eachTweet[1]))\n",
    "        tweetList.append(tweet)\n",
    "    \n",
    "    processedTweets = pd.Series(proTweets)  \n",
    "    return processedTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocess \n",
    "\n",
    "def preprocessTweet(tweet):                    \n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet)\n",
    "    tweet= re.sub('@[^\\s]+','AT_USER', tweet)\n",
    "    tweet = re.sub(r'#([^\\s]+)',r'\\l', tweet)                        \n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_test = preprocessTweets(X_test)\n",
    "X_train =X_train.apply(preprocessTweet)\n",
    "X_test=X_test.apply(preprocessTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(min_df=1,max_df=10)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "classifier.fit(X_train, Y_train)\n",
    "predicted = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item, labels in zip(X_test, predicted):\n",
    "    print (item + '==>' + labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
